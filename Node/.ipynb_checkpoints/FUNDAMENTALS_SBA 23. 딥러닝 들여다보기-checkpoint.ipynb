{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 다층 퍼셉트론 Numpy로 직접 구현해보기"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### 1. Tensorflow 기반 분류 모델 예시코드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense (Dense)                (None, 50)                39250     \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 10)                510       \n",
      "=================================================================\n",
      "Total params: 39,760\n",
      "Trainable params: 39,760\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/10\n",
      "1875/1875 [==============================] - 3s 1ms/step - loss: 0.4932 - accuracy: 0.8805\n",
      "Epoch 2/10\n",
      "1875/1875 [==============================] - 3s 2ms/step - loss: 0.2329 - accuracy: 0.9343\n",
      "Epoch 3/10\n",
      "1875/1875 [==============================] - 3s 1ms/step - loss: 0.1833 - accuracy: 0.9478\n",
      "Epoch 4/10\n",
      "1875/1875 [==============================] - 3s 1ms/step - loss: 0.1537 - accuracy: 0.9562\n",
      "Epoch 5/10\n",
      "1875/1875 [==============================] - 3s 2ms/step - loss: 0.1321 - accuracy: 0.9621\n",
      "Epoch 6/10\n",
      "1875/1875 [==============================] - 3s 2ms/step - loss: 0.1164 - accuracy: 0.9668\n",
      "Epoch 7/10\n",
      "1875/1875 [==============================] - 3s 1ms/step - loss: 0.1037 - accuracy: 0.9703\n",
      "Epoch 8/10\n",
      "1875/1875 [==============================] - 3s 1ms/step - loss: 0.0937 - accuracy: 0.9733\n",
      "Epoch 9/10\n",
      "1875/1875 [==============================] - 3s 1ms/step - loss: 0.0847 - accuracy: 0.9763\n",
      "Epoch 10/10\n",
      "1875/1875 [==============================] - 3s 2ms/step - loss: 0.0779 - accuracy: 0.9783\n",
      "313/313 - 0s - loss: 0.1043 - accuracy: 0.9680\n",
      "test_loss: 0.10432688891887665 \n",
      "test_accuracy: 0.9679999947547913\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Load data set\n",
    "mnist = keras.datasets.mnist\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "\n",
    "# Preprocessing\n",
    "x_train_norm, x_test_norm = x_train / 255.0, x_test / 255.0\n",
    "x_train_reshaped = x_train_norm.reshape(-1, x_train_norm.shape[1]*x_train_norm.shape[2])\n",
    "x_test_reshaped = x_test_norm.reshape(-1, x_test_norm.shape[1]*x_test_norm.shape[2])\n",
    "\n",
    "# Model\n",
    "model = keras.models.Sequential()\n",
    "model.add(keras.layers.Dense(50, activation = 'sigmoid', input_shape = (784,))) # 입력층 d = 784, 은닉층 레이어 H = 50\n",
    "model.add(keras.layers.Dense(10, activation = 'softmax')) # 출력층 레이어 K =10\n",
    "model.summary()\n",
    "\n",
    "# Model fit\n",
    "model.compile(optimizer = 'adam',\n",
    "             loss = 'sparse_categorical_crossentropy',\n",
    "             metrics = ['accuracy'])\n",
    "model.fit(x_train_reshaped, y_train, epochs = 10)\n",
    "\n",
    "# Model result\n",
    "test_loss, test_accuracy = model.evaluate(x_test_reshaped, y_test, verbose = 2)\n",
    "print(\"test_loss: {} \".format(test_loss))\n",
    "print(\"test_accuracy: {}\".format(test_accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Parameters/Weights\n",
    "\n",
    "* 입력층-은닉층, 은닉층-출력층 사이에는 사실 각각 행렬(Matrix)이 존재합니다. 예를 들어 입력값이 100개, 은닉 노드가 20개라면 사실 이 입력층-은닉층 사이에는 100x20의 형태를 가진 행렬이 존재합니다. 똑같이, MNIST 데이터처럼 10개의 클래스를 맞추는 문제를 풀기 위해 출력층이 10개의 노드를 가진다면 은닉층-출력층 사이에는 20x10의 형태를 가진 행렬이 존재하게 됩니다.\n",
    "\n",
    "* Parameter 혹은 Weight라고 부릅니다. 두 단어는 보통 같은 뜻으로 사용되지만, 실제로 Paraemter에는 위의 참고자료에서 다룬 bias 노드도 포함된다는 점만 유의해 주세요. 이때 인접한 레이어 사이에는 아래와 같은 관계가 성립합니다.\n",
    "  * y = W * X + b "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  MLP 기반 딥러닝 모델을 Numpy로 구현해보기"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 1) 입력층"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 784)\n",
      "(5, 784)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.]])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 입력층의 데이터 형태 (shape)\n",
    "print(x_train_reshaped.shape)\n",
    "\n",
    "X = x_train_reshaped[:5]\n",
    "print(X.shape)\n",
    "X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 2) Feed Forward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(784, 50)\n",
      "(50,)\n",
      "(5, 50)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([ 0.2612843 , -0.4013273 , -1.03136202, -0.60855203,  1.29834405,\n",
       "        0.02259547,  1.2016102 ,  0.60213712, -1.63256726, -1.39478777,\n",
       "       -1.05270691,  0.35211582, -0.16017535,  0.42467962, -0.37930823,\n",
       "       -1.42360352,  1.08810911, -0.8866059 , -0.5114341 ,  0.44231089,\n",
       "        1.70795199,  1.02747354,  0.68021111, -0.62451391, -0.36528738,\n",
       "        0.83843795,  1.08900454,  0.42731279,  0.99028257,  1.12324313,\n",
       "        1.00023888, -0.31703171,  0.3334153 ,  0.07851691,  0.32927863,\n",
       "        1.10676901,  0.06053865,  1.37631595, -0.09646048,  0.51913273,\n",
       "        0.15853866,  0.09680852,  0.54801193,  0.67788114,  1.45275654,\n",
       "       -0.12250306,  0.16199801,  0.43410442, -0.89390753, -0.96357116])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weight_init_std = 0.1\n",
    "input_size = 784\n",
    "hidden_size = 50\n",
    "\n",
    "# 인접 레이어간 관계를 나타내는 파라미터 W를 생성하고 random 초기화\n",
    "W1 = weight_init_std * np.random.randn(input_size, hidden_size)\n",
    "\n",
    "# 바이어스 파라미터 b를 생성하고 Zero로 초기화\n",
    "b1 = np.zeros(hidden_size)\n",
    "\n",
    "a1 = np.dot(X, W1) +b1 # 은닉층 출력\n",
    "\n",
    "\n",
    "print(W1.shape)\n",
    "print(b1.shape)\n",
    "print(a1.shape)\n",
    "a1[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 2-1) Activation Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.56495198 0.40099348 0.26282013 0.35238957 0.78555616 0.50564863\n",
      " 0.76881111 0.64614509 0.16347898 0.19864452 0.25870564 0.58713057\n",
      " 0.46004156 0.6046025  0.40629375 0.19409729 0.74802549 0.29181075\n",
      " 0.3748574  0.60880953 0.84657046 0.7364258  0.66378581 0.34875553\n",
      " 0.40968025 0.69813613 0.74819422 0.60523181 0.72914373 0.75458979\n",
      " 0.73110554 0.42139931 0.58259014 0.51961915 0.58158385 0.75152626\n",
      " 0.51513004 0.79839867 0.47590356 0.62694495 0.53955186 0.52418325\n",
      " 0.63367422 0.66326563 0.81042231 0.46941248 0.54041116 0.60685334\n",
      " 0.29030411 0.27616376]\n"
     ]
    }
   ],
   "source": [
    "# 첫번째 은닉층의 출력 a1에 sigmoid 적용\n",
    "\n",
    "def sigmoid(x):\n",
    "    return 1/(1+np.exp(-x))\n",
    "\n",
    "z1 = sigmoid(a1)\n",
    "print(z1[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 2-2) Dense layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def affine_layer_forward(X, W, b):\n",
    "    y = np.dot(X, W) + b\n",
    "    cache = (X, W, b)\n",
    "    return y, cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.6293621  -0.29038705  0.22399015  0.1068836  -0.04822379  0.16365023\n",
      " -0.29753613  0.17944764 -0.37526025 -0.13568257]\n"
     ]
    }
   ],
   "source": [
    "input_size = 784\n",
    "hidden_size = 50\n",
    "output_size = 10\n",
    "\n",
    "W1 = weight_init_std * np.random.randn(input_size, hidden_size)\n",
    "b1 = np.zeros(hidden_size)\n",
    "W2 = weight_init_std * np.random.randn(hidden_size, output_size)\n",
    "b2 = np.zeros(output_size)\n",
    "\n",
    "a1, cache1 = affine_layer_forward(X,W1,b1)\n",
    "z1 = sigmoid(a1)\n",
    "a2, cache2 = affine_layer_forward(z1, W2, b2)\n",
    "\n",
    "print(a2[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 3) pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*  softmax 함수란?\n",
    "\n",
    "softmax 함수는 K개의 값이 존재할 때 각각의 값의 편차를 확대시켜 큰 값은 상대적으로 더 크게, 작은 값은 상대적으로 더 작게 만든 다음에 normalization 시키는 함수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(x):\n",
    "    if x.ndim == 2:\n",
    "        x = x.T\n",
    "        x = x - np.max(x, axis = 0)\n",
    "        y = np.exp(x) / np.sum(np.exp(x), axis = 0)\n",
    "        return y.T\n",
    "    x = x - np.max(x)\n",
    "    return np.exp(x) / np.sum(np.exp(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.17671328, 0.07044127, 0.11781983, 0.1047996 , 0.08974233,\n",
       "       0.11092082, 0.06993948, 0.112687  , 0.06470938, 0.08222701])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_hat = softmax(a2)\n",
    "y_hat[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_hat = softmax(a2)\n",
    "y_hat[0]  # 10개의 숫자 중 하나일 확률이 되었습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 4) Loss Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., 0., 0., 1., 0., 0., 0., 0.],\n",
       "       [1., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 1., 0., 0., 0., 0., 0.],\n",
       "       [0., 1., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 1.]])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def _change_ont_hot_label(X, num_category):\n",
    "    T = np.zeros((X.size, num_category))\n",
    "    for idx, row in enumerate(T):\n",
    "        row[X[idx]] = 1\n",
    "    return T\n",
    "\n",
    "Y_digit = y_train[:5]\n",
    "t = _change_ont_hot_label(Y_digit, 10)\n",
    "t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 정답 라벨을 One-hot 인코딩하는 함수\n",
    "def _change_ont_hot_label(X, num_category):\n",
    "    T = np.zeros((X.size, num_category))\n",
    "    for idx, row in enumerate(T):\n",
    "        row[X[idx]] = 1\n",
    "        \n",
    "    return T\n",
    "\n",
    "Y_digit = y_train[:5]\n",
    "t = _change_ont_hot_label(Y_digit, 10)\n",
    "t     # 정답 라벨의 One-hot 인코딩"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.249290577065053"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def cross_entropy_error(y, t):\n",
    "    if y.ndim == 1:\n",
    "        t = t.reshape(1, t.size)\n",
    "        y = y.reshape(1, y.size)\n",
    "    \n",
    "    if t.size == y.size:\n",
    "        t = t.argmax(axis = 1)\n",
    "        \n",
    "    batch_size = y.shape[0]\n",
    "    return -np.sum(np.log(y[np.arange(batch_size), t])) / batch_size\n",
    "\n",
    "Loss = cross_entropy_error(y_hat, t)\n",
    "Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_entropy_error(y, t):\n",
    "    if y.ndim == 1:\n",
    "        t = t.reshape(1, t.size)\n",
    "        y = y.reshape(1, y.size)\n",
    "        \n",
    "    # 훈련 데이터가 원-핫 벡터라면 정답 레이블의 인덱스로 반환\n",
    "    if t.size == y.size:\n",
    "        t = t.argmax(axis=1)\n",
    "             \n",
    "    batch_size = y.shape[0]\n",
    "    return -np.sum(np.log(y[np.arange(batch_size), t])) / batch_size\n",
    "\n",
    "Loss = cross_entropy_error(y_hat, t)\n",
    "Loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 5) Gradient Descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.03534266,  0.01408825,  0.02356397,  0.02095992,  0.01794847,\n",
       "        -0.17781584,  0.0139879 ,  0.0225374 ,  0.01294188,  0.0164454 ],\n",
       "       [-0.16667751,  0.01328617,  0.02351717,  0.01644038,  0.02011891,\n",
       "         0.02448762,  0.01396723,  0.02391614,  0.01347546,  0.01746843],\n",
       "       [ 0.04401926,  0.01273787,  0.02124276,  0.01561742, -0.18226752,\n",
       "         0.02377224,  0.01527945,  0.01809542,  0.01501545,  0.01648765],\n",
       "       [ 0.03309813, -0.18456459,  0.02089384,  0.01759884,  0.017801  ,\n",
       "         0.02563827,  0.01488745,  0.02053569,  0.01749383,  0.01661754],\n",
       "       [ 0.03524385,  0.01552865,  0.02427028,  0.01390858,  0.01408146,\n",
       "         0.02615493,  0.01433308,  0.02065112,  0.01518332, -0.17935526]])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_num = y_hat.shape[0]\n",
    "dy = (y_hat - t) / batch_num\n",
    "dy # softmax값의 출력으로 Loss를 미분한 값"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_num = y_hat.shape[0]\n",
    "dy = (y_hat - t) / batch_num\n",
    "dy    # softmax값의 출력으로 Loss를 미분한 값"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "dW2 = np.dot(z1.T, dy)\n",
    "db2 = np.sum(dy, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 중간에 sigmoid가 한번 사용되었으므로, 활성화함수에 대한 gradient도 고려\n",
    "def sigmoid_grad(x):\n",
    "    return (1.0 - sigmoid(x)) * sigmoid(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "dz1 = np.dot(dy, W2.T)\n",
    "da1 = sigmoid_grad(a1) *dz1\n",
    "dW1 = np.dot(X.T, da1)\n",
    "db1 = np.sum(dz1, axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# learning_rate도 고려하여 파라미터 업데이트\n",
    "\n",
    "learning_rate =  0.1\n",
    "def update_params(W1, b1, W2, b2, dW1, db1, dW2, db2, learning_rate):\n",
    "    W1 = W1 - learning_rate*dW1\n",
    "    b1 = b1 - learning_rate*db1\n",
    "    W2 = W2 - learning_rate*dW2\n",
    "    b2 = b2 - learning_rate*db2\n",
    "    return W1, b1, W2, b2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 6) Backpropagation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def affine_layer_backward(dy, cache):\n",
    "    X, W, b = cache\n",
    "    dX = np.dot(dy, W.T)\n",
    "    dW = np.dot(X.T, dy)\n",
    "    db = np.sum(dy, axis=0)\n",
    "    return dX, dW, db"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 7) train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "W1 = weight_init_std * np.random.randn(input_size, hidden_size)\n",
    "b1 = np.zeros(hidden_size)\n",
    "W2 = weight_init_std * np.random.randn(hidden_size, output_size)\n",
    "b2 = np.zeros(output_size)\n",
    "\n",
    "def train_step(X, Y, W1, b1, W2, b2, learning_rate=0.1, verbose=False):\n",
    "    a1, cache1 = affine_layer_forward(X, W1, b1)\n",
    "    z1 = sigmoid(a1)\n",
    "    a2, cache2 = affine_layer_forward(z1, W2, b2)\n",
    "    y_hat = softmax(a2)\n",
    "    t = _change_ont_hot_label(Y, 10)\n",
    "    Loss = cross_entropy_error(y_hat, t)\n",
    "\n",
    "    if verbose:\n",
    "        print('---------')\n",
    "        print(y_hat)\n",
    "        print(t)\n",
    "        print('Loss: ', Loss)\n",
    "        \n",
    "    dy = (y_hat - t) / X.shape[0]\n",
    "    dz1, dW2, db2 = affine_layer_backward(dy, cache2)\n",
    "    da1 = sigmoid_grad(a1) * dz1\n",
    "    dX, dW1, db1 = affine_layer_backward(da1, cache1)\n",
    "    \n",
    "    W1, b1, W2, b2 = update_params(W1, b1, W2, b2, dW1, db1, dW2, db2, learning_rate)\n",
    "    \n",
    "    return W1, b1, W2, b2, Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------\n",
      "[[0.09158278 0.16784034 0.09042252 0.13037681 0.09589499 0.04362908\n",
      "  0.08620395 0.14247615 0.08663319 0.0649402 ]\n",
      " [0.09544838 0.15293797 0.08422071 0.13563836 0.1149616  0.03763848\n",
      "  0.088985   0.13027061 0.08906494 0.07083394]\n",
      " [0.11183921 0.15103187 0.07935425 0.11958546 0.13906404 0.04820051\n",
      "  0.08400501 0.1338072  0.08041449 0.05269795]\n",
      " [0.11127857 0.13201824 0.07651117 0.13362379 0.13483178 0.03892213\n",
      "  0.08463594 0.1368634  0.0959157  0.05539928]\n",
      " [0.10897784 0.13451369 0.08334208 0.11906637 0.1323748  0.05041743\n",
      "  0.08802842 0.12438213 0.09030912 0.06858814]]\n",
      "[[0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]]\n",
      "Loss:  2.4316945149755673\n",
      "---------\n",
      "[[0.10873859 0.18196954 0.08157167 0.10972236 0.10759103 0.05947351\n",
      "  0.07749992 0.11718746 0.07671829 0.07952763]\n",
      " [0.11652402 0.16329105 0.07564833 0.11461112 0.12830453 0.04882243\n",
      "  0.08045388 0.1071762  0.07900559 0.08616285]\n",
      " [0.12670445 0.16207902 0.07095811 0.09939114 0.16111295 0.06023304\n",
      "  0.07442119 0.11069856 0.07093573 0.06346582]\n",
      " [0.12765444 0.14867927 0.06867081 0.1121673  0.15084876 0.04980524\n",
      "  0.07543231 0.11356648 0.08530328 0.06787211]\n",
      " [0.12406887 0.14423686 0.07459353 0.10037331 0.14649707 0.06311835\n",
      "  0.07807935 0.10274141 0.07999543 0.08629581]]\n",
      "[[0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]]\n",
      "Loss:  2.230693964190511\n",
      "---------\n",
      "[[0.12288405 0.18977272 0.07302938 0.09353527 0.11535419 0.07739281\n",
      "  0.06918592 0.09817409 0.06766129 0.09301028]\n",
      " [0.13588547 0.16827188 0.06746539 0.09814452 0.13719406 0.0606691\n",
      "  0.07231764 0.0897626  0.06979918 0.10049015]\n",
      " [0.13777652 0.16830727 0.06314845 0.08370824 0.17958888 0.07251007\n",
      "  0.06569911 0.09321361 0.06244782 0.07360003]\n",
      " [0.14020831 0.16152634 0.06126456 0.09538521 0.16204039 0.06126673\n",
      "  0.06690702 0.09593339 0.07562968 0.07983837]\n",
      " [0.13560204 0.14960935 0.06644236 0.08578247 0.1559077  0.07605406\n",
      "  0.06900992 0.08647364 0.07071171 0.10440675]]\n",
      "[[0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]]\n",
      "Loss:  2.070887442949247\n",
      "---------\n",
      "[[0.13400856 0.19328534 0.06526289 0.08065068 0.11992844 0.09703202\n",
      "  0.06167613 0.08347628 0.05972066 0.104959  ]\n",
      " [0.15335588 0.16976058 0.06007844 0.08503965 0.14252491 0.07284234\n",
      "  0.06496472 0.07626843 0.06171451 0.11345053]\n",
      " [0.14548116 0.17138983 0.05623594 0.07138204 0.1950839  0.08466793\n",
      "  0.05809243 0.07968061 0.05512017 0.08286598]\n",
      " [0.14928889 0.17170897 0.05465016 0.08211372 0.169403   0.07299785\n",
      "  0.05938997 0.08227868 0.06718961 0.09097914]\n",
      " [0.14394817 0.15210184 0.05918825 0.07419987 0.16161404 0.08885202\n",
      "  0.0610574  0.07389233 0.06263846 0.12250763]]\n",
      "[[0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]]\n",
      "Loss:  1.9407140191534946\n",
      "---------\n",
      "[[0.14242184 0.19409338 0.05838005 0.07022841 0.12212049 0.11788726\n",
      "  0.05505313 0.07185273 0.05286761 0.1150951 ]\n",
      " [0.16906293 0.16915921 0.05357341 0.07444312 0.14523341 0.08494338\n",
      "  0.0584742  0.0655831  0.05473463 0.12479262]\n",
      " [0.15043071 0.17258104 0.05024446 0.06155943 0.20835065 0.09628986\n",
      "  0.05157183 0.06899848 0.04888038 0.09109317]\n",
      " [0.15547339 0.18020748 0.0488815  0.07147496 0.1739624  0.08461985\n",
      "  0.05289497 0.07149024 0.05995006 0.10104514]\n",
      " [0.14966657 0.15282791 0.05285793 0.06485451 0.16462643 0.10109273\n",
      "  0.05419369 0.06393698 0.05570964 0.14023359]]\n",
      "[[0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]]\n",
      "Loss:  1.832427168125981\n"
     ]
    }
   ],
   "source": [
    "X = x_train_reshaped[:5]\n",
    "Y = y_train[:5]\n",
    "\n",
    "for i in range(5):\n",
    "    W1, b1, W2, b2, _ = train_step(X, Y, W1, b1, W2, b2, learning_rate=0.1, verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 8) predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(W1, b1, W2, b2, X):\n",
    "    a1 = np.dot(X, W1) + b1\n",
    "    z1 = sigmoid(a1)\n",
    "    a2 = np.dot(z1, W2) + b2\n",
    "    y = softmax(a2)\n",
    "\n",
    "    return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.14858788, 0.19330781, 0.05235202, 0.0616807 , 0.12264183,\n",
       "       0.13936364, 0.04927637, 0.06249381, 0.04698714, 0.1233088 ])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# X = x_train[:100] 에 대해 모델 추론을 시도합니다. \n",
    "\n",
    "X = x_train_reshaped[:100]\n",
    "Y = y_test[:100]\n",
    "result = predict(W1, b1, W2, b2, X)\n",
    "result[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(W1, b1, W2, b2, x, y):\n",
    "    y_hat = predict(W1, b1, W2, b2, x)\n",
    "    y_hat = np.argmax(y_hat, axis=1)\n",
    "   # t = np.argmax(t, axis=1)\n",
    "\n",
    "    accuracy = np.sum(y_hat == y) / float(x.shape[0])\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.14858788 0.19330781 0.05235202 0.0616807  0.12264183 0.13936364\n",
      " 0.04927637 0.06249381 0.04698714 0.1233088 ]\n",
      "[0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      "0.12\n"
     ]
    }
   ],
   "source": [
    "acc = accuracy(W1, b1, W2, b2, X, Y)\n",
    "\n",
    "t = _change_ont_hot_label(Y, 10)\n",
    "print(result[0])\n",
    "print(t[0])\n",
    "print(acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense (Dense)                (None, 50)                39250     \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 10)                510       \n",
      "=================================================================\n",
      "Total params: 39,760\n",
      "Trainable params: 39,760\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/10\n",
      "1875/1875 [==============================] - 3s 2ms/step - loss: 0.4987 - accuracy: 0.8832\n",
      "Epoch 2/10\n",
      "1875/1875 [==============================] - 2s 1ms/step - loss: 0.2300 - accuracy: 0.9353\n",
      "Epoch 3/10\n",
      "1875/1875 [==============================] - 2s 1ms/step - loss: 0.1787 - accuracy: 0.9488\n",
      "Epoch 4/10\n",
      "1875/1875 [==============================] - 2s 1ms/step - loss: 0.1493 - accuracy: 0.9572\n",
      "Epoch 5/10\n",
      "1875/1875 [==============================] - 2s 1ms/step - loss: 0.1278 - accuracy: 0.9640\n",
      "Epoch 6/10\n",
      "1875/1875 [==============================] - 2s 1ms/step - loss: 0.1120 - accuracy: 0.9680\n",
      "Epoch 7/10\n",
      "1875/1875 [==============================] - 2s 1ms/step - loss: 0.1000 - accuracy: 0.9711\n",
      "Epoch 8/10\n",
      "1875/1875 [==============================] - 2s 1ms/step - loss: 0.0899 - accuracy: 0.9742\n",
      "Epoch 9/10\n",
      "1875/1875 [==============================] - 2s 1ms/step - loss: 0.0817 - accuracy: 0.9773\n",
      "Epoch 10/10\n",
      "1875/1875 [==============================] - 2s 1ms/step - loss: 0.0752 - accuracy: 0.9795\n",
      "313/313 - 0s - loss: 0.0977 - accuracy: 0.9692\n",
      "test_loss: 0.09767191857099533 \n",
      "test_accuracy: 0.9692000150680542\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# MNIST 데이터를 로드. 다운로드하지 않았다면 다운로드까지 자동으로 진행됩니다. \n",
    "mnist = keras.datasets.mnist\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()   \n",
    "\n",
    "# 모델에 맞게 데이터 가공\n",
    "x_train_norm, x_test_norm = x_train / 255.0, x_test / 255.0\n",
    "x_train_reshaped = x_train_norm.reshape(-1, x_train_norm.shape[1]*x_train_norm.shape[2])\n",
    "x_test_reshaped = x_test_norm.reshape(-1, x_test_norm.shape[1]*x_test_norm.shape[2])\n",
    "\n",
    "# 딥러닝 모델 구성 - 2 Layer Perceptron\n",
    "model=keras.models.Sequential()\n",
    "model.add(keras.layers.Dense(50, activation='sigmoid', input_shape=(784,)))  # 입력층 d=784, 은닉층 레이어 H=50\n",
    "model.add(keras.layers.Dense(10, activation='softmax'))   # 출력층 레이어 K=10\n",
    "model.summary()\n",
    "\n",
    "# 모델 구성과 학습\n",
    "model.compile(optimizer='adam',\n",
    "             loss='sparse_categorical_crossentropy',\n",
    "             metrics=['accuracy'])\n",
    "model.fit(x_train_reshaped, y_train, epochs=10)\n",
    "\n",
    "# 모델 테스트 결과\n",
    "test_loss, test_accuracy = model.evaluate(x_test_reshaped,y_test, verbose=2)\n",
    "print(\"test_loss: {} \".format(test_loss))\n",
    "print(\"test_accuracy: {}\".format(test_accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 784)\n",
      "(5, 784)\n"
     ]
    }
   ],
   "source": [
    "# 입력층 데이터의 모양(shape)\n",
    "print(x_train_reshaped.shape)\n",
    "\n",
    "# 테스트를 위해 x_train_reshaped의 앞 5개의 데이터를 가져온다.\n",
    "X = x_train_reshaped[:5]\n",
    "print(X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(784, 50)\n",
      "(50,)\n",
      "(5, 50)\n"
     ]
    }
   ],
   "source": [
    "weight_init_std = 0.1\n",
    "input_size = 784\n",
    "hidden_size=50\n",
    "\n",
    "# 인접 레이어간 관계를 나타내는 파라미터 W를 생성하고 random 초기화\n",
    "W1 = weight_init_std * np.random.randn(input_size, hidden_size)  \n",
    "# 바이어스 파라미터 b를 생성하고 Zero로 초기화\n",
    "b1 = np.zeros(hidden_size)\n",
    "\n",
    "a1 = np.dot(X, W1) + b1   # 은닉층 출력\n",
    "\n",
    "print(W1.shape)\n",
    "print(b1.shape)\n",
    "print(a1.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.26900943, -0.74499413,  0.76569976,  1.52396907,  0.77407353,\n",
       "        0.5106225 ,  0.09994509,  0.19317798,  0.49943508,  0.69454337,\n",
       "       -0.2193685 ,  0.2404075 ,  0.30131396,  0.26771715,  0.88435629,\n",
       "        0.67215659, -0.0294649 ,  0.00544126, -0.04665228,  0.81957332,\n",
       "        1.0683334 ,  1.47917226, -0.37593638,  0.96372063,  0.96274987,\n",
       "       -0.834748  , -0.13030156, -0.13883221,  0.08527631, -0.15032689,\n",
       "       -0.13430864,  0.50450818, -0.64520428, -1.549126  ,  0.0989828 ,\n",
       "       -0.79408716,  2.55269759,  1.40927037,  1.021984  , -0.24234614,\n",
       "       -1.22118648,  0.40248027, -0.68045444, -0.28597097,  0.31430273,\n",
       "        0.92634547,  0.69817375,  0.94473074, -0.57103992,  0.47605183])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 첫번째 데이터의 은닉층 출력을 확인해 봅시다.  50dim의 벡터가 나오나요?\n",
    "a1[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.5668497  0.32191303 0.68258993 0.8211222  0.68440142 0.62495239\n",
      " 0.52496549 0.54814487 0.62232656 0.66697686 0.44537675 0.55981407\n",
      " 0.57476369 0.56653238 0.70772414 0.66198589 0.49263431 0.50136031\n",
      " 0.48833904 0.69414576 0.74427984 0.81444752 0.40710737 0.72386612\n",
      " 0.72367204 0.30264207 0.46747062 0.46534759 0.52130617 0.46248889\n",
      " 0.46647322 0.62351818 0.34407105 0.17521254 0.52472551 0.31129175\n",
      " 0.92775453 0.80365084 0.73535888 0.43970826 0.22772772 0.59928343\n",
      " 0.33615988 0.42899052 0.57793516 0.71633327 0.66778274 0.72005425\n",
      " 0.3609969  0.61681514]\n"
     ]
    }
   ],
   "source": [
    "# sigmoid 함수 구현\n",
    "\n",
    "def sigmoid(x):\n",
    "    return 1/(1+np.exp(-x))\n",
    "\n",
    "z1 = sigmoid(a1)\n",
    "print(z1[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 단일 레이어 구현 함수\n",
    "\n",
    "def affine_layer_forward(X,W,b):\n",
    "    y = np.dot(X,W) + b\n",
    "    cache = (X,W,b)\n",
    "    return y, cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.07734357  0.02036746  0.03429832 -0.09785393 -0.33464087  0.37359658\n",
      "  0.48617341  0.1699204   0.08121021 -0.68471173]\n"
     ]
    }
   ],
   "source": [
    "input_size = 784\n",
    "hidden_size = 50\n",
    "output_size = 10\n",
    "\n",
    "W1 = weight_init_std * np.random.randn(input_size, hidden_size)\n",
    "b1 = np.zeros(hidden_size)\n",
    "W2 = weight_init_std * np.random.randn(hidden_size, output_size)\n",
    "b2 = np.zeros(output_size)\n",
    "\n",
    "a1, cache1 = affine_layer_forward(X, W1, b1)\n",
    "z1 = sigmoid(a1)\n",
    "a2, cache2 = affine_layer_forward(z1, W2, b2)    # z1이 다시 두번째 레이어의 입력이 됩니다. \n",
    "\n",
    "print(a2[0])  # 최종 출력이 output_size만큼의 벡터가 되었습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(x):\n",
    "    if x.ndim == 2:\n",
    "        x = x.T\n",
    "        x = x - np.max(x, axis=0)\n",
    "        y = np.exp(x) / np.sum(np.exp(x), axis=0)\n",
    "        return y.T \n",
    "\n",
    "    x = x - np.max(x) # 오버플로 대책\n",
    "    return np.exp(x) / np.sum(np.exp(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.08851644, 0.09760214, 0.09897133, 0.08671943, 0.06843546,\n",
       "       0.13895199, 0.15550927, 0.11334682, 0.10372489, 0.04822223])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_hat = softmax(a2)\n",
    "y_hat[0]  # 10개의 숫자 중 하나일 확률이 되었습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., 0., 0., 1., 0., 0., 0., 0.],\n",
       "       [1., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 1., 0., 0., 0., 0., 0.],\n",
       "       [0., 1., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 1.]])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 정답 라벨을 One-hot 인코딩하는 함수\n",
    "def _change_ont_hot_label(X, num_category):\n",
    "    T = np.zeros((X.size, num_category))\n",
    "    for idx, row in enumerate(T):\n",
    "        row[X[idx]] = 1\n",
    "        \n",
    "    return T\n",
    "\n",
    "Y_digit = y_train[:5]\n",
    "t = _change_ont_hot_label(Y_digit, 10)\n",
    "t     # 정답 라벨의 One-hot 인코딩"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.08851644 0.09760214 0.09897133 0.08671943 0.06843546 0.13895199\n",
      " 0.15550927 0.11334682 0.10372489 0.04822223]\n",
      "[0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]\n"
     ]
    }
   ],
   "source": [
    "print(y_hat[0])\n",
    "print(t[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.4199465184469715"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def cross_entropy_error(y, t):\n",
    "    if y.ndim == 1:\n",
    "        t = t.reshape(1, t.size)\n",
    "        y = y.reshape(1, y.size)\n",
    "        \n",
    "    # 훈련 데이터가 원-핫 벡터라면 정답 레이블의 인덱스로 반환\n",
    "    if t.size == y.size:\n",
    "        t = t.argmax(axis=1)\n",
    "             \n",
    "    batch_size = y.shape[0]\n",
    "    return -np.sum(np.log(y[np.arange(batch_size), t])) / batch_size\n",
    "\n",
    "Loss = cross_entropy_error(y_hat, t)\n",
    "Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.01770329,  0.01952043,  0.01979427,  0.01734389,  0.01368709,\n",
       "        -0.1722096 ,  0.03110185,  0.02266936,  0.02074498,  0.00964445],\n",
       "       [-0.18351893,  0.01901633,  0.01621143,  0.01469751,  0.01483514,\n",
       "         0.02658811,  0.03665958,  0.02123464,  0.0212283 ,  0.01304789],\n",
       "       [ 0.01973813,  0.01816138,  0.01960985,  0.01663035, -0.1853926 ,\n",
       "         0.0283451 ,  0.03105554,  0.02230375,  0.01859942,  0.01094909],\n",
       "       [ 0.01306896, -0.17956437,  0.01962564,  0.01974692,  0.01393162,\n",
       "         0.02832278,  0.02939372,  0.02172824,  0.02189062,  0.01185586],\n",
       "       [ 0.01312517,  0.01865193,  0.01970109,  0.01827955,  0.01593988,\n",
       "         0.0263097 ,  0.03234102,  0.02019484,  0.02244129, -0.18698447]])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_num = y_hat.shape[0]\n",
    "dy = (y_hat - t) / batch_num\n",
    "dy    # softmax값의 출력으로 Loss를 미분한 값"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.08960477, -0.05408093,  0.0529602 ,  0.04836863, -0.06908652,\n",
       "        -0.02187472,  0.09086445,  0.06065396,  0.0589838 , -0.07718409],\n",
       "       [-0.09960359, -0.09009493,  0.06192372,  0.05701928, -0.01770513,\n",
       "        -0.07549234,  0.1055538 ,  0.07088424,  0.06955594, -0.08204098],\n",
       "       [-0.07054283, -0.06388465,  0.06342962,  0.05770504, -0.08669214,\n",
       "        -0.07640982,  0.10664529,  0.07241279,  0.06963729, -0.07230059],\n",
       "       [-0.00370311, -0.0480881 ,  0.02342693,  0.02173891, -0.04112274,\n",
       "         0.02448275,  0.03816521,  0.02585368,  0.02564127, -0.0663948 ],\n",
       "       [-0.00432232, -0.06806661,  0.04547689,  0.04130017, -0.12399126,\n",
       "        -0.02342553,  0.07371025,  0.05131757,  0.04838205, -0.04038122],\n",
       "       [-0.02853415, -0.03694057,  0.0300887 ,  0.02746236, -0.05062225,\n",
       "        -0.01577773,  0.05032331,  0.03414255,  0.03295264, -0.04309487],\n",
       "       [-0.09338662, -0.03827291,  0.05696451,  0.05170948, -0.05853396,\n",
       "        -0.08111507,  0.09759104,  0.06553326,  0.06324167, -0.06373139],\n",
       "       [-0.0770746 ,  0.00228739,  0.0442292 ,  0.03962462, -0.10518318,\n",
       "        -0.02246381,  0.07646339,  0.05093829,  0.04849756, -0.05731886],\n",
       "       [-0.02364921, -0.00911669,  0.02944639,  0.02654178, -0.08737812,\n",
       "         0.01760477,  0.04945872,  0.03312797,  0.03184528, -0.06788088],\n",
       "       [-0.03394661, -0.04383108,  0.03940106,  0.03586069, -0.03314033,\n",
       "        -0.08663743,  0.06553358,  0.04501559,  0.04315342, -0.03140889],\n",
       "       [-0.00254277, -0.04768491,  0.04554379,  0.04158036, -0.09466655,\n",
       "         0.00465122,  0.0743014 ,  0.05048341,  0.04925245, -0.12091839],\n",
       "       [-0.03894368, -0.06182905,  0.04268151,  0.0393028 , -0.00484754,\n",
       "        -0.07345619,  0.07113908,  0.04831133,  0.04748354, -0.06984182],\n",
       "       [-0.06688627, -0.08238715,  0.04395076,  0.04076348, -0.02197424,\n",
       "        -0.01061408,  0.07467313,  0.04994598,  0.04946998, -0.07694159],\n",
       "       [-0.07213049, -0.06453476,  0.06257801,  0.05718313, -0.09282475,\n",
       "        -0.01546871,  0.10564349,  0.07085684,  0.06911371, -0.12041646],\n",
       "       [-0.09504314, -0.04114578,  0.04886623,  0.04444119, -0.06038887,\n",
       "        -0.04472584,  0.08446878,  0.05645986,  0.05446563, -0.04739807],\n",
       "       [-0.09927889, -0.06199352,  0.05422427,  0.04947696, -0.07618063,\n",
       "        -0.0304761 ,  0.09325597,  0.06250395,  0.06031239, -0.05184439],\n",
       "       [-0.04412699, -0.06441212,  0.05995946,  0.05471784, -0.07929337,\n",
       "        -0.05715022,  0.09969621,  0.06771621,  0.06568468, -0.1027917 ],\n",
       "       [-0.02429034, -0.08721103,  0.03671162,  0.03411985, -0.04857064,\n",
       "         0.01258916,  0.06049806,  0.04119277,  0.04047822, -0.06551767],\n",
       "       [-0.08132294, -0.06067858,  0.06438324,  0.05864511, -0.09344828,\n",
       "        -0.04482519,  0.10897612,  0.07336214,  0.07100852, -0.09610013],\n",
       "       [-0.07360827, -0.06720437,  0.056977  ,  0.05209566, -0.08951169,\n",
       "        -0.01035216,  0.09647594,  0.06478959,  0.06295908, -0.09262077],\n",
       "       [-0.0188635 , -0.02662411,  0.03623438,  0.03258777, -0.12158003,\n",
       "         0.00194147,  0.05994405,  0.04105284,  0.03858563, -0.04327848],\n",
       "       [-0.12111594, -0.05457488,  0.06271769,  0.0574049 , -0.0312958 ,\n",
       "        -0.06017245,  0.10850994,  0.07194307,  0.07070579, -0.10412231],\n",
       "       [-0.03040179, -0.09099437,  0.0426479 ,  0.03960777, -0.03683179,\n",
       "        -0.00509337,  0.07050236,  0.0478352 ,  0.04726492, -0.08453682],\n",
       "       [-0.00024357, -0.03457714,  0.0246635 ,  0.02257307, -0.04892474,\n",
       "        -0.00395673,  0.04002508,  0.02743486,  0.02662366, -0.05361798],\n",
       "       [-0.06239096, -0.04536403,  0.04343476,  0.03968911, -0.04720167,\n",
       "        -0.03428787,  0.07388032,  0.04957373,  0.04825192, -0.06558531],\n",
       "       [-0.06122584, -0.04462329,  0.06543867,  0.05942716, -0.10031166,\n",
       "        -0.05415822,  0.10983609,  0.07407449,  0.07174486, -0.12020225],\n",
       "       [-0.00387789, -0.01139425,  0.03428756,  0.03087157, -0.06777701,\n",
       "        -0.04891896,  0.05609521,  0.0384643 ,  0.03677712, -0.06452764],\n",
       "       [-0.01758268, -0.06544541,  0.03840987,  0.03531143, -0.04028563,\n",
       "        -0.0428165 ,  0.06297779,  0.04326815,  0.04201364, -0.05585065],\n",
       "       [-0.07742751, -0.05703201,  0.04642422,  0.04252338, -0.03757464,\n",
       "        -0.04574947,  0.07939176,  0.05327699,  0.0518583 , -0.05569103],\n",
       "       [-0.08069788, -0.06333392,  0.0423045 ,  0.0386259 , -0.05765963,\n",
       "        -0.04341889,  0.07260075,  0.04918786,  0.04692948, -0.00453818],\n",
       "       [-0.07541071, -0.00977607,  0.05331567,  0.0479924 , -0.08606523,\n",
       "        -0.07133106,  0.09099939,  0.06114578,  0.058554  , -0.06942417],\n",
       "       [-0.10198987, -0.05410514,  0.05544035,  0.05061243, -0.03343937,\n",
       "        -0.08100221,  0.09538978,  0.06393361,  0.0620753 , -0.05691489],\n",
       "       [-0.03194134, -0.01144001,  0.02778695,  0.02497295, -0.0676598 ,\n",
       "        -0.02180576,  0.04695828,  0.03182006,  0.03011759, -0.02880894],\n",
       "       [-0.03173596, -0.02674815,  0.03277132,  0.02962087, -0.09061243,\n",
       "         0.00189343,  0.05500675,  0.03727679,  0.03544759, -0.04292023],\n",
       "       [-0.08739916, -0.01146086,  0.03814272,  0.03450326, -0.08180216,\n",
       "         0.01907313,  0.0670825 ,  0.04396402,  0.04257312, -0.06467657],\n",
       "       [-0.06702808, -0.03257116,  0.04550435,  0.04116277, -0.0723345 ,\n",
       "        -0.05905897,  0.07747291,  0.05241352,  0.04999326, -0.03555409],\n",
       "       [-0.03374552, -0.02429903,  0.03696406,  0.03342599, -0.08499585,\n",
       "        -0.01338972,  0.06200057,  0.04193688,  0.04014813, -0.0580455 ],\n",
       "       [-0.01748536, -0.09807725,  0.03690737,  0.03417524, -0.02393912,\n",
       "        -0.06966695,  0.05993399,  0.04205566,  0.04032366, -0.00422723],\n",
       "       [-0.03559674, -0.08964555,  0.05211233,  0.04813157, -0.02576001,\n",
       "        -0.05623764,  0.08616007,  0.05863049,  0.05770361, -0.09549813],\n",
       "       [-0.08301067, -0.08624833,  0.05925844,  0.05462534, -0.04351694,\n",
       "        -0.0233448 ,  0.10055998,  0.06724202,  0.06632701, -0.11189206],\n",
       "       [-0.00609838, -0.05894046,  0.02524142,  0.02304528, -0.09290717,\n",
       "         0.01751795,  0.04088236,  0.02865229,  0.0266798 , -0.00407308],\n",
       "       [-0.04144845, -0.01712272,  0.03358636,  0.0305666 , -0.015385  ,\n",
       "        -0.05869115,  0.05693235,  0.03817093,  0.03736711, -0.06397602],\n",
       "       [-0.07925646, -0.04177337,  0.04996276,  0.04546549, -0.07500099,\n",
       "        -0.02568664,  0.08551462,  0.05719667,  0.05537149, -0.07179358],\n",
       "       [ 0.00799412, -0.08079871,  0.06582894,  0.05979651, -0.1266848 ,\n",
       "        -0.09154147,  0.10609285,  0.0738817 ,  0.07034563, -0.08491478],\n",
       "       [-0.10734485, -0.07275701,  0.07336493,  0.06701807, -0.07365312,\n",
       "        -0.06472819,  0.12493002,  0.08376295,  0.08158958, -0.11218237],\n",
       "       [-0.0828467 , -0.03355538,  0.05026539,  0.04595822, -0.04323433,\n",
       "        -0.01907804,  0.08648066,  0.05708343,  0.05641329, -0.11748655],\n",
       "       [-0.013439  , -0.05460927,  0.03150581,  0.02886286, -0.0668258 ,\n",
       "        -0.0042044 ,  0.05164066,  0.0354913 ,  0.03409577, -0.04251792],\n",
       "       [-0.01834484, -0.04269932,  0.05997115,  0.05411199, -0.13259993,\n",
       "        -0.07162397,  0.09840736,  0.06773696,  0.06429705, -0.07925645],\n",
       "       [-0.02599018, -0.03495183,  0.05415792,  0.04900305, -0.08627874,\n",
       "        -0.0810606 ,  0.0894401 ,  0.06117719,  0.05867168, -0.08416859],\n",
       "       [-0.07340142, -0.0751492 ,  0.0623159 ,  0.05677989, -0.10574246,\n",
       "        -0.04312886,  0.10491105,  0.0711893 ,  0.06834191, -0.0661161 ]])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dW2 = np.dot(z1.T, dy)    \n",
    "dW2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "dW2 = np.dot(z1.T, dy)\n",
    "db2 = np.sum(dy, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid_grad(x):\n",
    "    return (1.0 - sigmoid(x)) * sigmoid(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "dz1 = np.dot(dy, W2.T)\n",
    "da1 = sigmoid_grad(a1) * dz1\n",
    "dW1 = np.dot(X.T, da1)\n",
    "db1 = np.sum(dz1, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.1\n",
    "\n",
    "def update_params(W1, b1, W2, b2, dW1, db1, dW2, db2, learning_rate):\n",
    "    W1 = W1 - learning_rate*dW1\n",
    "    b1 = b1 - learning_rate*db1\n",
    "    W2 = W2 - learning_rate*dW2\n",
    "    b2 = b2 - learning_rate*db2\n",
    "    return W1, b1, W2, b2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
