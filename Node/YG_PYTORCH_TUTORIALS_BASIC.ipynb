{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PYTORCH?\n",
    "\n",
    "Python 기반의 과학 연산 패키지로 다음과 같은 두 집단을 대상으로 합니다:\n",
    "\n",
    "* NumPy를 대체하면서 GPU를 이용한 연산이 필요한 경우\n",
    "* 최대한의 유연성과 속도를 제공하는 딥러닝 연구 플랫폼이 필요한 경우"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 시작하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[9.2755e-39, 1.0561e-38, 7.8061e-39],\n",
      "        [9.2755e-39, 8.4490e-39, 1.0102e-38],\n",
      "        [9.0919e-39, 1.0102e-38, 8.9082e-39],\n",
      "        [8.4489e-39, 9.6429e-39, 8.4490e-39],\n",
      "        [9.6429e-39, 9.2755e-39, 1.0286e-38]])\n"
     ]
    }
   ],
   "source": [
    "# 초기화되지 않은 5x3 행렬을 생성합니다:\n",
    "x = torch.empty(5,3)\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.1095, 0.5156, 0.6988],\n",
      "        [0.8760, 0.1545, 0.0918],\n",
      "        [0.4903, 0.0691, 0.9781],\n",
      "        [0.7454, 0.3286, 0.5765],\n",
      "        [0.7253, 0.6313, 0.3568]])\n"
     ]
    }
   ],
   "source": [
    "# 무작위로 초기화된 행렬을 생성합니다:\n",
    "x = torch.rand(5,3)\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0, 0, 0],\n",
      "        [0, 0, 0],\n",
      "        [0, 0, 0],\n",
      "        [0, 0, 0],\n",
      "        [0, 0, 0]])\n"
     ]
    }
   ],
   "source": [
    "# dtype이 long이고 0으로 채워진 행렬을 생성합니다:\n",
    "x = torch.zeros(5,3, dtype = torch.long)\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([5.5000, 3.0000])\n"
     ]
    }
   ],
   "source": [
    "# 데이터로부터 tensor를 직접 생성합니다\n",
    "x = torch.tensor([5.5,3])\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 1., 1.],\n",
      "        [1., 1., 1.],\n",
      "        [1., 1., 1.],\n",
      "        [1., 1., 1.],\n",
      "        [1., 1., 1.]], dtype=torch.float64)\n",
      "tensor([[ 0.6018, -0.5243,  2.1538],\n",
      "        [-0.5393,  1.0955, -1.5897],\n",
      "        [-1.0502, -0.4310,  0.1149],\n",
      "        [ 0.2870,  1.5991, -0.8101],\n",
      "        [-0.1683,  1.5675, -0.9646]])\n"
     ]
    }
   ],
   "source": [
    "# 기존 tensor를 바탕으로 새로운 tensor를 만듭니다. 이들 메소드(method)는 사용자로부터 새로운 값을 제공받지 않은 한, \n",
    "# 입력 tensor의 속성들(예. dtype)을 재사용합니다.\n",
    "\n",
    "x = x.new_ones(5,3, dtype = torch.double)\n",
    "print(x)\n",
    "\n",
    "x = torch.randn_like(x, dtype = torch.float)\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([5, 3])\n"
     ]
    }
   ],
   "source": [
    "# 행렬의 크기를 구합니다\n",
    "print(x.size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 연산(Operations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 1.1220,  0.2961,  2.9013],\n",
      "        [ 0.2438,  1.1031, -1.2197],\n",
      "        [-0.9781,  0.2612,  0.5278],\n",
      "        [ 1.1255,  1.9653,  0.0999],\n",
      "        [ 0.5181,  1.7943, -0.3811]])\n"
     ]
    }
   ],
   "source": [
    "# 덧셈: 문법1\n",
    "y = torch.rand(5, 3)\n",
    "print(x + y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 1.1220,  0.2961,  2.9013],\n",
      "        [ 0.2438,  1.1031, -1.2197],\n",
      "        [-0.9781,  0.2612,  0.5278],\n",
      "        [ 1.1255,  1.9653,  0.0999],\n",
      "        [ 0.5181,  1.7943, -0.3811]])\n"
     ]
    }
   ],
   "source": [
    "# 덧셈: 문법2\n",
    "print(torch.add(x, y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 1.1220,  0.2961,  2.9013],\n",
      "        [ 0.2438,  1.1031, -1.2197],\n",
      "        [-0.9781,  0.2612,  0.5278],\n",
      "        [ 1.1255,  1.9653,  0.0999],\n",
      "        [ 0.5181,  1.7943, -0.3811]])\n"
     ]
    }
   ],
   "source": [
    "# 덧셈: 결과 tensor를 인자로 제공\n",
    "result = torch.empty(5, 3)\n",
    "torch.add(x, y, out = result)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.5202, 0.8204, 0.7475],\n",
      "        [0.7832, 0.0076, 0.3701],\n",
      "        [0.0721, 0.6922, 0.4129],\n",
      "        [0.8385, 0.3662, 0.9100],\n",
      "        [0.6865, 0.2268, 0.5835]])\n"
     ]
    }
   ],
   "source": [
    "# 덧셈: 바꿔치기(in-place) 방식\n",
    "# y에 x 더하기\n",
    "y.add(x)\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 4]) torch.Size([16]) torch.Size([2, 8])\n"
     ]
    }
   ],
   "source": [
    "# 크기 변경: tensor의 크기(size)나 모양(shape)을 변경하고 싶다면 torch.view 를 사용합니다\n",
    "x = torch.randn(4, 4)\n",
    "y = x.view(16)\n",
    "z = x.view(-1, 8)\n",
    "print(x.size(), y.size(), z.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.7407])\n",
      "0.740706741809845\n"
     ]
    }
   ],
   "source": [
    "# 만약 tensor에 하나의 값만 존재한다면, .item() 을 사용하면 숫자 값을 얻을 수 있습니다.\n",
    "x = torch.randn(1)\n",
    "print(x)\n",
    "print(x.item())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NumPy 변환(Bridge)\n",
    "* Torch Tensor를 NumPy 배열(array)로 변환하거나, 그 반대로 하는 것은 매우 쉽습니다.\n",
    "\n",
    "* (Torch Tensor가 CPU 상에 있다면) Torch Tensor와 NumPy 배열은 메모리 공간을 공유하기 때문에, 하나를 변경하면 다른 하나도 변경됩니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1., 1., 1., 1., 1.])\n"
     ]
    }
   ],
   "source": [
    "# Torch Tensor를 NumPy 배열로 변환하기\n",
    "a = torch.ones(5)\n",
    "print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1. 1. 1. 1. 1.]\n"
     ]
    }
   ],
   "source": [
    "b = a.numpy()\n",
    "print(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([2., 2., 2., 2., 2.])\n",
      "[2. 2. 2. 2. 2.]\n"
     ]
    }
   ],
   "source": [
    "a.add_(1)\n",
    "print(a)\n",
    "print(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2. 2. 2. 2. 2.]\n",
      "tensor([2., 2., 2., 2., 2.], dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "# NumPy 배열을 Torch Tensor로 변환하기\n",
    "import numpy as np\n",
    "a = np.ones(5)\n",
    "b = torch.from_numpy(a)\n",
    "np.add(a, 1, out = a)\n",
    "print(a)\n",
    "print(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CharTensor를 제외한 CPU 상의 모든 Tensor는 NumPy로 변환할 수 있고, (NumPy에서 Tensor로의) 반대 변환도 가능합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CUDA Tensors\n",
    "* .to 메소드를 사용하여 Tensor를 어떠한 장치로도 옮길 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1.7407], device='cuda:0')\n",
      "tensor([1.7407], dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "    y = torch.ones_like(x, device = device)\n",
    "    x = x.to(device)\n",
    "    z = x + y\n",
    "    print(z)\n",
    "    print(z.to(\"cpu\", torch.double))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AUTOGRAD: 자동 미분\n",
    "\n",
    "PyTorch의 모든 신경망의 중심에는 autograd 패키지가 있습니다. 먼저 이것을 가볍게 살펴본 뒤, 첫번째 신경망을 학습시켜보겠습니다.\n",
    "\n",
    "autograd 패키지는 Tensor의 모든 연산에 대해 자동 미분을 제공합니다. 이는 실행-기반-정의(define-by-run) 프레임워크로, 이는 코드를 어떻게 작성하여 실행하느냐에 따라 역전파가 정의된다는 뜻이며, 역전파는 학습 과정의 매 단계마다 달라집니다.\n",
    "\n",
    "더 간단한 용어로 몇 가지 예를 살펴보겠습니다.\n",
    "\n",
    "\n",
    "## Tensor\n",
    "\n",
    "패키지의 중심에는 torch.Tensor 클래스가 있습니다. 만약 .requires_grad 속성을 True 로 설정하면, 그 tensor에서 이뤄진 모든 연산들을 추적(track)하기 시작합니다. 계산이 완료된 후 .backward() 를 호출하여 모든 변화도(gradient)를 자동으로 계산할 수 있습니다. 이 Tensor의 변화도는 .grad 속성에 누적됩니다.\n",
    "\n",
    "Tensor가 기록을 추적하는 것을 중단하게 하려면, .detach() 를 호출하여 연산 기록으로부터 분리(detach)하여 이후 연산들이 추적되는 것을 방지할 수 있습니다.\n",
    "\n",
    "기록을 추적하는 것(과 메모리를 사용하는 것)을 방지하기 위해, 코드 블럭을 with torch.no_grad(): 로 감쌀 수 있습니다. 이는 특히 변화도(gradient)는 필요없지만, requires_grad=True 가 설정되어 학습 가능한 매개변수를 갖는 모델을 평가(evaluate)할 때 유용합니다.\n",
    "\n",
    "Autograd 구현에서 매우 중요한 클래스가 하나 더 있는데, 이것은 바로 Function 클래스입니다.\n",
    "\n",
    "Tensor 와 Function 은 서로 연결되어 있으며, 모든 연산 과정을 부호화(encode)하여 순환하지 않는 그래프(acyclic graph)를 생성합니다. 각 tensor는 .grad_fn 속성을 갖고 있는데, 이는 Tensor 를 생성한 Function 을 참조하고 있습니다. (단, 사용자가 만든 Tensor는 예외로, 이 때 grad_fn 은 None 입니다.)\n",
    "\n",
    "도함수를 계산하기 위해서는 Tensor 의 .backward() 를 호출하면 됩니다. 만약 Tensor 가 스칼라(scalar)인 경우(예. 하나의 요소 값만 갖는 등)에는 backward 에 인자를 정해줄 필요가 없습니다. 하지만 여러 개의 요소를 갖고 있을 때는 tensor의 모양을 gradient 의 인자로 지정할 필요가 있습니다.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 1.],\n",
      "        [1., 1.]], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "# tensor를 생성하고 requires_grad=True 를 설정하여 연산을 기록합니다.\n",
    "x = torch.ones(2,2, requires_grad = True)\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[3., 3.],\n",
      "        [3., 3.]], grad_fn=<AddBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# tensor에 연산을 수행합니다.\n",
    "y = x + 2\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<AddBackward0 object at 0x00000264CFEA4948>\n"
     ]
    }
   ],
   "source": [
    "# y 는 연산의 결과로 생성된 것이므로 grad_fn 을 갖습니다.\n",
    "print(y.grad_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[27., 27.],\n",
      "        [27., 27.]], grad_fn=<MulBackward0>) tensor(27., grad_fn=<MeanBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# y 에 다른 연산을 수행합니다.\n",
    "z = y * y * 3\n",
    "out = z.mean()\n",
    "\n",
    "print(z, out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n",
      "True\n",
      "<SumBackward0 object at 0x00000264CFEAB148>\n"
     ]
    }
   ],
   "source": [
    "# .requires_grad_( ... ) 는 기존 Tensor의 requires_grad 값을 바꿔치기 (in-place)하여 변경합니다. \n",
    "# 입력값이 지정되지 않으면 기본값은 False 입니다.\n",
    "\n",
    "a = torch.randn(2, 2)\n",
    "a = ((a*3) / (a-1))\n",
    "print(a.requires_grad)\n",
    "a.requires_grad_(True)\n",
    "print(a.requires_grad)\n",
    "b = (a * a).sum()\n",
    "print(b.grad_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 변화도(Gradient)\n",
    "\n",
    "이제 역전파(backprop)를 해보겠습니다. out 은 하나의 스칼라 값만 갖고 있기 때문에,\n",
    "\n",
    "out.backward() 는 out.backward(torch.tensor(1.)) 과 동일합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "out.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[4.5000, 4.5000],\n",
      "        [4.5000, 4.5000]])\n"
     ]
    }
   ],
   "source": [
    "print(x.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 일반적으로, torch.autograd 는 벡터-야코비안 곱을 계산하는 엔진입니다. \n",
    "* 벡터-야코비안 곱의 이러한 특성은 스칼라가 아닌 출력을 갖는 모델에 외부 변화도를 제공(feed)하는 것을 매우 편리하게 해줍니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([  867.9491,   455.8497, -1497.9010], grad_fn=<MulBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# 벡터-야코비안 곱의 예제를 살펴보도록 하겠습니다:\n",
    "\n",
    "x = torch.randn(3, requires_grad = True)\n",
    "y = x * 2\n",
    "while y.data.norm() < 1000:\n",
    "    y = y * 2\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1.0240e+02, 1.0240e+03, 1.0240e-01])\n"
     ]
    }
   ],
   "source": [
    "# 이 경우 y 는 더 이상 스칼라 값이 아닙니다. torch.autograd 는 전체 야코비안을 직접 계산할수는 없지만, \n",
    "# 벡터-야코비안 곱은 간단히 backward 에 해당 벡터를 인자로 제공하여 얻을 수 있습니다\n",
    "\n",
    "v = torch.tensor([0.1, 1.0, 0.0001], dtype = torch.float)\n",
    "y.backward(v)\n",
    "\n",
    "print(x.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "True\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "# 또한 with torch.no_grad(): 로 코드 블럭을 감싸서 autograd가 \n",
    "# .requires_grad=True 인 Tensor들의 연산 기록을 추적하는 것을 멈출 수 있습니다.\n",
    "\n",
    "print(x.requires_grad)\n",
    "print((x ** 2).requires_grad)\n",
    "\n",
    "with torch.no_grad():\n",
    "    print((x ** 2).requires_grad)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "False\n",
      "tensor(True)\n"
     ]
    }
   ],
   "source": [
    "# 또는 .detach() 를 호출하여 내용물(content)은 같지만 require_grad가 다른 새로운 Tensor를 가져옵니다:\n",
    "\n",
    "print(x.requires_grad)\n",
    "y = x.detach()\n",
    "print(y.requires_grad)\n",
    "print(x.eq(y).all())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
